{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHmxidfJvI7XAEAKaQPPQT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CR-MLE/Machine-Learning/blob/main/week2_Computer_vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo0V7si38do3",
        "outputId": "eba5648d-5e8d-418a-e1e2-408f891dfa1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load Fashion MNIST data\n",
        "fmist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "# these are 28x28 pixels clathing images and already available in the keras library"
      ],
      "metadata": {
        "id": "5E_oKOjF8mDa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(training_images,training_labels), (test_images,test_labels) = fmist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R43kvxv3ED7i",
        "outputId": "fbb8db37-d5e3-4fb6-eb3d-65cb55462b9e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# there are 60K pics starting from 0\n",
        "index = 24\n",
        "#set number of rows when printing\n",
        "np.set_printoptions(linewidth=320)\n",
        "#print the label and image\n",
        "\n",
        "print(f'Label: {training_labels[index]}')\n",
        "print(f'\\n image pixel array: \\n {training_images[index]}')\n",
        "\n",
        "#print the image\n",
        "plt.imshow(training_images[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "id": "E-26yDgBF_E_",
        "outputId": "f6f28309-19b8-4605-d918-53158b99b6bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 4\n",
            "\n",
            " image pixel array: \n",
            " [[  0   0   0   0   0   0   0   0   0   0  90 156 177 182 196 176 117   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   2   0   0 239 253 239 214 226 214 231 245 248   0   0   1   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  36 241 203 219 192 190 193 207 200 232 135   0   0   2   0   0   0   0   0   0]\n",
            " [  0   0   0   0   1   0   0  33 167 227 229 234 228 234 244 215 211 214 208 120   0   0   2   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0 157 227 207 207 229 229 232 207 241 227 235 224 203 221 176   0   0   2   0   0   0   0]\n",
            " [  0   0   0   0   0  65 218 189 192 187 196 189 188 202 181 195 222 219 190 180 217 125   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0 158 213 196 196 191 188 186 179 192 177 192 126 155 193 189 200 166   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0 163 217 211 176 188 188 186 183 196 180 191 165 170 216 204 200 195   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0 183 215 234 185 182 186 183 184 198 181 185 195 188 203 245 207 209  37   0   0   0   0   0]\n",
            " [  0   0   0   0   0 196 215 240 209 188 183 186 186 190 186 188 188 183 212 253 205 217  64   0   0   0   0   0]\n",
            " [  0   0   0   0   0 235 213 246 209 190 190 195 194 192 192 192 196 189 209 252 206 217 109   0   0   0   0   0]\n",
            " [  0   0   0   0  18 246 209 253 212 201 209 195 202 204 203 206 207 201 214 251 205 216 160   0   0   0   0   0]\n",
            " [  0   0   0   0  48 249 208 253 217 196 197 191 195 193 194 196 196 194 214 247 206 216 189   0   0   0   0   0]\n",
            " [  0   0   0   0  71 244 206 255 216 195 196 194 197 196 196 195 203 197 197 238 216 215 184   0   0   0   0   0]\n",
            " [  0   0   0   0  91 248 206 255 211 194 197 193 196 195 195 195 199 201 191 247 217 214 198   0   0   0   0   0]\n",
            " [  0   0   0   0 109 224 206 237 187 195 198 193 197 195 194 194 197 203 197 193 211 217 215   0   0   0   0   0]\n",
            " [  0   0   0   0 118 230 205 222 166 204 195 195 198 196 193 192 196 200 206 161 207 219 220   0   0   0   0   0]\n",
            " [  0   0   0   0 110 234 210 183 147 217 189 193 197 197 194 191 194 198 212 136 213 221 220   5   0   0   0   0]\n",
            " [  0   0   0   0 109 239 209 167 146 219 186 192 196 198 194 191 192 195 223 117 206 227 219  31   0   0   0   0]\n",
            " [  0   0   0   0 105 241 212 142 146 218 182 191 193 196 196 192 190 190 232 100 166 238 218  48   0   0   0   0]\n",
            " [  0   0   0   0  99 240 219 115 152 212 180 190 191 192 198 195 187 185 235  99 133 244 216  63   0   0   0   0]\n",
            " [  0   0   0   0  95 238 225  86 163 203 181 188 190 191 195 195 189 181 226 112 133 245 216  83   0   0   0   0]\n",
            " [  0   0   0   0 102 238 228  79 165 198 182 186 189 192 192 191 191 182 217 113 131 250 209  95   0   0   0   0]\n",
            " [  0   0   0   0 113 234 239 103 155 199 182 187 191 192 191 190 191 183 212 130 128 243 203 103   0   0   0   0]\n",
            " [  0   0   0   0 114 220 251 106 144 187 180 184 187 188 187 186 185 183 190 144 103 236 202 110   0   0   0   0]\n",
            " [  0   0   0   0 114 208 252  71 176 202 195 198 202 202 201 201 201 197 202 178  81 233 205 116   0   0   0   0]\n",
            " [  0   0   0   0 109 238 255  38 105 166 156 159 161 162 159 156 156 155 170 112  44 241 224 136   0   0   0   0]\n",
            " [  0   0   0   0  33 177 179  16   0   0   0   0   0   0   0   0   0   0   0   0   0 163 165  53   0   0   0   0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcde37fc310>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for the  model learning , it is advisable to keep values in 0 and 1 , hence dividing by 255\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.00\n",
        "\n",
        "#print(training_images[4])"
      ],
      "metadata": {
        "id": "nAA7QeWUPVAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c158b0c8-cf7d-4d25-88ae-0aae430ed28b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00039985 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.00290657 0.00316801 0.00287582 0.00049212 0.         0.         0.         0.00039985 0.00333718 0.00347559 0.00301423 0.00016917 0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.00295271 0.00349097 0.00359862 0.00373702 0.0035371  0.00226067 0.00367551 0.00372165 0.00359862 0.00335256 0.00321415 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.00266052 0.00346021 0.00330642 0.00358324 0.00390619 0.         0.00298347 0.00369089 0.00333718 0.00339869 0.00292195 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.00213764 0.00352172 0.00326028 0.00347559 0.00392157 0.         0.00249135 0.00392157 0.00327566 0.00347559 0.00307574 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.00150711 0.00356786 0.00324491 0.00330642 0.0038293  0.00070742 0.00249135 0.00378316 0.00329104 0.0035371  0.00286044 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.00107651 0.00350634 0.00327566 0.00338331 0.00344483 0.00387543 0.00367551 0.00336794 0.00333718 0.00355248 0.00262976 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.00069204 0.00341407 0.00329104 0.00335256 0.0033218  0.00322953 0.00330642 0.00333718 0.0031065  0.00344483 0.00264514 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.00036909 0.00390619 0.00329104 0.00322953 0.00324491 0.00329104 0.00330642 0.00326028 0.00312188 0.00339869 0.00256824 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00390619 0.0033218  0.00330642 0.00333718 0.00333718 0.0033218  0.0033218  0.00316801 0.00346021 0.00230681 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00379854 0.0033218  0.00329104 0.00333718 0.0033218  0.00329104 0.00326028 0.00312188 0.00347559 0.0020915  0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00376778 0.0033218  0.00329104 0.0033218  0.00333718 0.00330642 0.00324491 0.00313725 0.00346021 0.00192234 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00379854 0.0033218  0.00329104 0.00333718 0.00338331 0.00333718 0.00327566 0.00312188 0.00341407 0.00226067 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00381392 0.0033218  0.00330642 0.00335256 0.00341407 0.0033218  0.00329104 0.00318339 0.00335256 0.00275279 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.0038293  0.0033218  0.00333718 0.00336794 0.00341407 0.00333718 0.00329104 0.00322953 0.00330642 0.00324491 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.00058439 0.00392157 0.00329104 0.00335256 0.00336794 0.00344483 0.00335256 0.00330642 0.00324491 0.00324491 0.00355248 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.00121492 0.00349097 0.00321415 0.00336794 0.00336794 0.00349097 0.00336794 0.00330642 0.00327566 0.00316801 0.00390619 0.00089196 0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.00224529 0.00347559 0.00324491 0.00338331 0.00336794 0.00350634 0.00335256 0.00330642 0.0033218  0.00315263 0.00336794 0.00250673 0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.0031065  0.00339869 0.00329104 0.00339869 0.00336794 0.00355248 0.00335256 0.00330642 0.00335256 0.00327566 0.00326028 0.00338331 0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.00359862 0.00333718 0.0033218  0.00338331 0.00336794 0.00359862 0.00333718 0.00330642 0.00335256 0.0033218  0.00342945 0.00379854 0.00010765 0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.00026144 0.00390619 0.00326028 0.00336794 0.00336794 0.00338331 0.00358324 0.00329104 0.0033218  0.00336794 0.00341407 0.00235294 0.00366013 0.00089196 0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.00101499 0.00392157 0.00319877 0.00338331 0.00336794 0.00341407 0.00370627 0.00338331 0.00335256 0.00335256 0.00335256 0.00295271 0.00372165 0.00152249 0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.00218378 0.00361399 0.00312188 0.00335256 0.0033218  0.00355248 0.00372165 0.00346021 0.00358324 0.00336794 0.00329104 0.0033218  0.00366013 0.00221453 0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.00272203 0.00381392 0.00349097 0.00352172 0.00324491 0.00392157 0.00116878 0.         0.00379854 0.00373702 0.0035371  0.0035371  0.0038293  0.00287582 0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.00155325 0.00370627 0.00350634 0.00350634 0.00338331 0.00392157 0.00098424 0.         0.00373702 0.00364475 0.0035371  0.00349097 0.00370627 0.00218378 0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.00392157 0.00372165 0.00341407 0.00335256 0.00392157 0.00095348 0.         0.00342945 0.00366013 0.00346021 0.00366013 0.00392157 0.00047674 0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.00069204 0.00392157 0.00372165 0.00361399 0.00392157 0.00129181 0.         0.00378316 0.00392157 0.00372165 0.00392157 0.00107651 0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.0009381  0.00156863 0.00258362 0.00038447 0.         0.00213764 0.00247597 0.00113802 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build classification model\n",
        "model=tf.keras.Sequential([tf.keras.layers.Flatten(),\n",
        "                           tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                           tf.keras.layers.Dense(10, activation =tf.nn.softmax)])\n",
        "#softmax regression is multiclass  classification. "
      ],
      "metadata": {
        "id": "Gw98hU77PsGn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Declare sample inputs and convert to a tensor\n",
        "\n",
        "inputs = np.array([[1.0,3.0,4.0,2.0]])\n",
        "inputs = tf.convert_to_tensor(inputs)\n",
        "print(f'inputs to softmax function:{inputs.numpy()}')\n",
        "\n",
        "#feed the input tensor to softmax function\n",
        "\n",
        "output = tf.keras.activations.softmax(inputs)\n",
        "print(f'output  by softmax: {output.numpy()}')\n",
        "\n",
        "#get the sum of all values after the softmax\n",
        "\n",
        "sum  = tf.reduce_sum(output)\n",
        "print(f'sum of output: {sum}')\n",
        "\n",
        "#Get the index with highest value\n",
        "prediction = np.argmax(output)\n",
        "\n",
        "print(f'class with higest probability: {prediction}')"
      ],
      "metadata": {
        "id": "sflXqnhCxe8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148f7ea3-4a36-46f4-a9d7-8980792103ff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs to softmax function:[[1. 3. 4. 2.]]\n",
            "output  by softmax: [[0.0320586  0.23688282 0.64391426 0.08714432]]\n",
            "sum of output: 1.0\n",
            "class with higest probability: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v17GQeZACgyO",
        "outputId": "cf38575a-ab64-480d-f6bc-1dc33eb6d528"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4827 - accuracy: 0.8298\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4672 - accuracy: 0.8349\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4554 - accuracy: 0.8402\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4453 - accuracy: 0.8430\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4367 - accuracy: 0.8471\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4297 - accuracy: 0.8486\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4230 - accuracy: 0.8511\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4177 - accuracy: 0.8530\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4124 - accuracy: 0.8552\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4075 - accuracy: 0.8571\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcddc44f790>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_images, test_labels)\n",
        "#evaluate function gives error and accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fi2YhArDA06",
        "outputId": "f68a7b3d-4443-4723-d7d2-79f9fd6aa855"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4426 - accuracy: 0.8394\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.44259077310562134, 0.8393999934196472]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#excericse 1\n",
        "classifications = model.predict(test_images)\n",
        "#output1 = classifications[0].numpy()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ag8JA_jEzxr",
        "outputId": "a377bc93-5fab-4097-efd9-79119859c85d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'predicted value of first image: {np.argmax(classifications[0])}')\n",
        "print(f'actual image: {test_labels[0]}')\n",
        "#when I increased the number of epocs to 10 , the actual and predicted image was same."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWAfQaeqFN4_",
        "outputId": "837f0ba7-fb77-4e5b-b9ce-4e7d01ad9ac9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted value of first image: 9\n",
            "actual image: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self,epoch,logs={}):\n",
        "    #halt the training when it reaches the accuracy\n",
        "\n",
        "    if (logs.get('loss')<0.4):\n",
        "      print(\"\\n stop training as loss is reduced below 0.4\")\n",
        "      self.model.stop_training  = True\n",
        "callbacks  = myCallback()\n"
      ],
      "metadata": {
        "id": "lil-iZsqQ35l"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fminst =tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images,training_labels),(test_images,test_labels) = fminst.load_data()\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(512,activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10,activation=tf.nn.softmax)])\n",
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy')\n",
        "model.fit(training_images, training_labels, epochs=5,callbacks = [callbacks])\n",
        "model.evaluate(test_images, test_labels)\n",
        "classifications = model.predict(test_images)\n",
        "\n",
        "print(f'predicted value of first image: {np.argmax(classifications[0])}')\n",
        "print(f'actual image: {test_labels[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGGsxa0MLr8b",
        "outputId": "1455b4a9-5ad9-4ef0-c036-946913db5221"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4763\n",
            "Epoch 2/5\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.3592\n",
            " stop training as loss is reduced below 0.4\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3591\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3931\n",
            "313/313 [==============================] - 1s 3ms/step\n",
            "predicted value of first image: 9\n",
            "actual image: 9\n"
          ]
        }
      ]
    }
  ]
}